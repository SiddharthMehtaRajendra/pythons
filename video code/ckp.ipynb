{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import confusion_matrix\n",
    "import ckp_attn_model\n",
    "from ckp_attn_model import BaselineModel\n",
    "import data_loader\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## fer map\n",
    "emotion_dict = {0: \"Anger\", 1: \"Contempt\", 2: \"Disgust\", 3: \"Fear\", 4: \"Happy\", 5: \"Sadness\", 6: \"Surprise\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (conv1): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (conv4): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (conv6): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten()\n",
       "  (fc1): LinearBlock(\n",
       "    (linblock): Sequential(\n",
       "      (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (fc2): LinearBlock(\n",
       "    (linblock): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (fc3): LinearBlock(\n",
       "    (linblock): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (attlayer): AttentionLayer(\n",
       "    (attention_fclayer): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc4): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaselineModel(num_blocks=3)\n",
    "model.load_state_dict(torch.load('models/fer_cnn_ckp_attention_best_model.pt', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# loader = data_loader.get_test_loader() \n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_num, (feats, labels) in enumerate(loader):\n",
    "#         outputs = model(feats)\n",
    "        \n",
    "#         _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "#         pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "#         all_preds.append(pred_labels[0].item())\n",
    "#         all_labels.append(labels[0].item())\n",
    "        \n",
    "\n",
    "# metrics.confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "\n",
    "# print(all_preds)\n",
    "# print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # img = Image.open('imgs/surprise.png')\n",
    "# # img_pil = torchvision.transforms.Resize((48,48))(img)\n",
    "# # img = torchvision.transforms.ToTensor()(img_pil)\n",
    "# # img = torchvision.transforms.Normalize(mean=[0.485], std=[0.229])(img)\n",
    "# # img = img.unsqueeze(dim=0)\n",
    "# # prhttp://localhost:8888/notebooks/DL/project/CK%2B/pythons/video%20code/ckp.ipynb#int(\"img \", img)\n",
    "\n",
    "# # # img_pil.show()\n",
    "\n",
    "\n",
    "# frame = cv2.imread('imgs/surprise.png')\n",
    "# print(\"frame type: \", type(frame))\n",
    "# test = Image.fromarray(frame)\n",
    "# test_img_pil = torchvision.transforms.Resize((48,48))(test)\n",
    "# test = torchvision.transforms.ToTensor()(test_img_pil)\n",
    "# test = torchvision.transforms.Normalize(mean=[0.485], std=[0.229])(test)\n",
    "# print(\"test: \", test)\n",
    "\n",
    "\n",
    "# img = Image.open('imgs/surprise.png')\n",
    "# img = Image.open('imgs/surprise.png')\n",
    "# img_pil = torchvision.transforms.Resize((48,48))(img)\n",
    "# img = torchvision.transforms.ToTensor()(img_pil)\n",
    "# img = torchvision.transforms.Normalize(mean=[0.485], std=[0.229])(img)\n",
    "# print(\"IMage: \", img)\n",
    "\n",
    "# # print(\"img: \", type(img))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('frame shape: ', (670, 464, 3))\n",
      "('img ', torch.Size([1, 1, 48, 48]))\n",
      "tensor([[-0.5199,  3.4965, -5.7553,  4.2202, -3.4968, -3.0499, -3.5751]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# # Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Read the input image\n",
    "frame = cv2.imread('imgs/random_smile.png')\n",
    "\n",
    "# Convert into grayscale\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "print(\"frame shape: \", frame.shape)\n",
    "\n",
    "# Detect faces\n",
    "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "\n",
    "# Draw the rectangle around each face\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "    roi_gray = gray[y:y + h, x:x + w]\n",
    "    \n",
    "    \n",
    "    cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), 0), 0)\n",
    "    cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n",
    "    \n",
    "#     print(\"img shape: \", cropped_img.shape)\n",
    "#     print(type(cropped_img))\n",
    "\n",
    "    \n",
    "#     img = Image.open('imgs/surprise.png')\n",
    "    img = Image.fromarray(frame)\n",
    "    img_pil = torchvision.transforms.Resize((48,48))(img)\n",
    "    img = torchvision.transforms.ToTensor()(img_pil)\n",
    "    if img.shape[0] == 3:\n",
    "        img = torchvision.transforms.Grayscale(num_output_channels=1)(img_pil)\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "    \n",
    "    \n",
    "    img = torchvision.transforms.Normalize(mean=[0.485], std=[0.229])(img)\n",
    "    img = img.unsqueeze(dim=0)\n",
    "    print(\"img \", img.shape)\n",
    "    prediction = model(img)\n",
    "        \n",
    "    print(prediction)\n",
    "    print(torch.argmax(prediction).item())\n",
    "#     print(emotion_dict)\n",
    "    cv2.putText(frame, emotion_dict[int(torch.argmax(prediction).item())], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    \n",
    "# Display\n",
    "cv2.imshow('frame', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # To capture video from webcam. \n",
    "# cap = cv2.VideoCapture(0)\n",
    "# # # To use a video file as input \n",
    "# # cap = cv2.VideoCapture('filename.mp4')\n",
    "\n",
    "# # # Load the cascade\n",
    "# face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     # Read the frame\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     # Convert to grayscale\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Detect the faces\n",
    "#     faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "#     # Draw the rectangle around each face\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "#         roi_gray = gray[y:y + h, x:x + w]\n",
    "#         cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), 0), 0)\n",
    "\n",
    "#         cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n",
    "\n",
    "# #         cropped_img = torch.from_numpy(cropped_img)\n",
    "# #         cropped_img = cropped_img.float()\n",
    "\n",
    "#         img = Image.fromarray(frame)\n",
    "#         img_pil = torchvision.transforms.Resize((48,48))(img)\n",
    "#         img = torchvision.transforms.ToTensor()(img_pil)\n",
    "#         if img.shape[0] == 3:\n",
    "#             img = torchvision.transforms.Grayscale(num_output_channels=1)(img_pil)\n",
    "# #             img.show()\n",
    "#             img = torchvision.transforms.ToTensor()(img)\n",
    "\n",
    "#         img = torchvision.transforms.Normalize(mean=[0.485], std=[0.229])(img)\n",
    "#         img = img.unsqueeze(dim=0)\n",
    "#         prediction = model(img)\n",
    "        \n",
    "#         print(prediction)\n",
    "#         cv2.putText(frame, emotion_dict[int(torch.argmax(prediction).item())], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "#     # Display\n",
    "#     cv2.imshow('frame', frame)\n",
    "    \n",
    "#     # Stop if q key is pressed\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "        \n",
    "# # Release the VideoCapture object\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
