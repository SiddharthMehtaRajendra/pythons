{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdRpCepJz7XV",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from torchvision import models\n",
    "import csv\n",
    "import tripletloss\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34BLnzCbzd6w",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#981\n",
    "train_size= 589\n",
    "val_size= 196\n",
    "test_size = 196\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "config = Config(\n",
    "    center_loss = False,\n",
    "    triplet_loss = True,\n",
    "    num_classes = 11,\n",
    "    feat_dim = 11,\n",
    "    lr_cent = 0.5,\n",
    "    closs_weight = 0.5,\n",
    "    margin = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ebc2817f0b05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_data_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training.csv'"
     ]
    }
   ],
   "source": [
    "with open(\"training.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter = ',')\n",
    "    for row in reader:\n",
    "        train_data_map[row[0]] = row[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'validation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-86a83f8d9c07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mval_data_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'validation.csv'"
     ]
    }
   ],
   "source": [
    "with open(\"validation.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter = ',')\n",
    "    for row in reader:\n",
    "        val_data_map[row[0]] = row[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXc2tVVQ7Cds",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_data(datadir, train_data_map, val_data_map):\n",
    "    train_img_list = []\n",
    "    val_img_list = []\n",
    "    extensions = [\"BMP\", \"JPEG\", \"JPG\", \"Jpeg\", \"PNG\", \"bmp\", \"jpeg\", \"jpg\", \"png\", \"tif\"]\n",
    "    for root, directories, filenames in os.walk(datadir): \n",
    "        for filename in filenames:\n",
    "            extenstion = filename.split(\".\")[-1]\n",
    "            if extenstion in extensions:  \n",
    "                \n",
    "                filei = os.path.join(root, filename)\n",
    "                split_filename = filei.split(\"/\")\n",
    "                filename = split_filename[-2] + \"/\" + split_filename[-1]\n",
    "                if filename in train_data_map:\n",
    "                    train_img_list.append(filei)\n",
    "                elif filename in val_data_map:\n",
    "                    val_img_list.append(filei)\n",
    "               \n",
    "    return train_img_list, val_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_img_list, val_img_list = parse_data(\"Manually_Annotated_Images\", train_data_map, val_data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_list = train_img_list[:10]\n",
    "len(train_img_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1572058491942,
     "user": {
      "displayName": "Mohit Grover",
      "photoUrl": "",
      "userId": "17020987855024013058"
     },
     "user_tz": 420
    },
    "id": "tTjycRTo9_KN",
    "outputId": "95d0dd64-73dd-47cc-8d4e-993b43bd77d0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_list, data_map):\n",
    "        self.img_list = img_list\n",
    "        self.data_map = data_map\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_file = self.img_list[index]\n",
    "        img = Image.open(img_file)\n",
    "        img_h = 224\n",
    "        img_w = 224\n",
    "        img = torchvision.transforms.Resize((img_h,img_w))(img)\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        split_filename = img_file.split(\"/\")\n",
    "        filename = split_filename[-2] + \"/\" + split_filename[-1]\n",
    "        \n",
    "        label = self.data_map[filename]\n",
    "        return img, int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataset(train_img_list, train_data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_len = int(np.floor(0.8 * len(train_data)))\n",
    "val_data_len = len(train_data) - train_data_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_data, (train_data_len, val_data_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name in train_data_map:\n",
    "#     labels.add(train_data_map[file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = ImageDataset(val_img_list, val_data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integeral value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-053f38592604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             raise ValueError(\"num_samples should be a positive integeral \"\n\u001b[0;32m---> 64\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"replacement should be a boolean value, but got \"\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integeral value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"training_baseline_affectnet_part_8.log\" ,\n",
    "                            filemode=\"a+\")\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Wxrwfckzd7B",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "                          nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=kernel_size, stride=stride),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        layers = []\n",
    "        num_classes = 11\n",
    "        channels = [3, 64, 128, 256] # this needs to be modified according to num_blocks\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            layers.append(ConvBlock(C_in=channels[i], C_out=channels[i+1], kernel_size=5, stride=1))\n",
    "        \n",
    "        layers.append(nn.Dropout(p=0.25))\n",
    "        \n",
    "        layers.append(Flatten())\n",
    "        \n",
    "        layers.append(nn.Linear(256*24*24, 512))\n",
    "        \n",
    "        layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        layers.append(nn.Linear(512, num_classes))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6150,
     "status": "ok",
     "timestamp": 1572056426800,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "re2Kdhiazd7C",
    "outputId": "bfb5ad37-62c5-4bf4-e910-452f67ca8b17",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f4445c28de06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimizer_tripletloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion_tripletloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_cent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     40\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     41\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model = BaselineModel(num_blocks=3)\n",
    "\n",
    "criterion_tripletloss = tripletloss.TripletLoss(config.margin)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "optimizer_tripletloss = torch.optim.Adam(criterion_tripletloss.parameters(), lr=config.lr_cent)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRLEOoHdzd7E",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                logger.info('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    " \n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_loss, train_accuracy = test_classify_loss(model,train_dataloader)\n",
    "        test_loss, test_accuracy = test_classify_loss(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        logger.info('Epoch: {}\\tTrain Loss: {}\\tTrain Acc: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, train_loss,train_accuracy, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_loss(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracies = 0\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            accuracies += float(torch.sum(torch.eq(pred_labels, labels)).item())\n",
    "            total+=float(len(labels))\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracies/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_closs(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            outputs = model(feats)\n",
    "            \n",
    "            loss = criterion(outputs, labels.long())\n",
    "            c_loss = criterion_closs(outputs, labels.long())\n",
    "            loss = loss + config.closs_weight * c_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / config.closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                logger.info('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    " \n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_loss, train_accuracy = test_classify_closs(model,train_dataloader)\n",
    "        test_loss, test_accuracy = test_classify_closs(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        logger.info('Epoch: {}\\tTrain Loss: {}\\tTrain Acc: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, train_loss,train_accuracy, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_closs(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracies = 0\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            c_loss = criterion_closs(outputs, labels.long())\n",
    "            loss = loss + config.closs_weight * c_loss\n",
    "            \n",
    "            accuracies += float(torch.sum(torch.eq(pred_labels, labels)).item())\n",
    "            total+=float(len(labels))\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracies/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tripletloss(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             optimizer_closs.zero_grad()\n",
    "            \n",
    "            outputs = model(feats)\n",
    "            \n",
    "            t_loss = criterion_tripletloss(outputs, labels.long())\n",
    "            loss = t_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_tripletloss.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                logger.info('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    " \n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_loss, train_accuracy = test_classify_tripletloss(model,train_dataloader)\n",
    "        test_loss, test_accuracy = test_classify_tripletloss(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        logger.info('Epoch: {}\\tTrain Loss: {}\\tTrain Acc: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, train_loss,train_accuracy, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_tripletloss(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracies = 0\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            t_loss = criterion_tripletloss(outputs, labels.long())\n",
    "            loss = t_loss\n",
    "            \n",
    "            accuracies += float(torch.sum(torch.eq(pred_labels, labels)).item())\n",
    "            total+=float(len(labels))\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracies/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30341,
     "status": "error",
     "timestamp": 1572056451006,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "ACkvD4NVzd7G",
    "outputId": "9e3fc8f9-69ae-45f6-de28-8bdbbeb73637",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c9463e3d3ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tripletloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses, eval_losses, eval_accs = train_tripletloss(model,5, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6AXVsznzd7J",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-035ac1e326df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch Number'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_loss.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses)\n",
    "plt.savefig(\"training_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(eval_accs)\n",
    "plt.savefig(\"val_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a27ee5e06a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                shuffle=False, num_workers=8)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, \n",
    "                                               shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = test_classify_closs(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_cnn (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
