{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdRpCepJz7XV",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34BLnzCbzd6w",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "#981\n",
    "train_size= 589\n",
    "val_size= 196\n",
    "test_size = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXc2tVVQ7Cds",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_data(datadir, label_map):\n",
    "    img_list = []\n",
    "    file_list = []\n",
    "    \n",
    "    for root, directories, filenames in os.walk(datadir):      \n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.png'):\n",
    "                \n",
    "                filei = os.path.join(root, filename)\n",
    "                file_ids = filename.split('_')\n",
    "                file_id = file_ids[0] + '_' + file_ids[1]\n",
    "                if file_id in label_map:\n",
    "                    img_list.append(filei)\n",
    "    \n",
    "    return img_list[:train_size], img_list[train_size:train_size+val_size], img_list[train_size+val_size: train_size+val_size+test_size]\n",
    "\n",
    "\n",
    "\n",
    "def parse_emotion_data(datadir):\n",
    "    em_map = {}\n",
    "    file_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):\n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.txt'):\n",
    "                   \n",
    "                f = open(root +  \"/\" + filename, 'r')\n",
    "                lines = []\n",
    "                for line in f.readlines():\n",
    "                    lines.append(line)\n",
    "                value = lines[0]\n",
    "                f.close()\n",
    "                \n",
    "                keys = filename.split('_')\n",
    "                key = keys[0] + '_' + keys[1]\n",
    "                em_map[key] = int(float(value.strip())) - 1\n",
    "                \n",
    "    return em_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "label_map = parse_emotion_data(\"Emotion\")\n",
    "train_img_list, val_img_list, test_img_list = parse_data(\"cohn-kanade-images\", label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1572058491942,
     "user": {
      "displayName": "Mohit Grover",
      "photoUrl": "",
      "userId": "17020987855024013058"
     },
     "user_tz": 420
    },
    "id": "tTjycRTo9_KN",
    "outputId": "95d0dd64-73dd-47cc-8d4e-993b43bd77d0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, label_map, train = False):\n",
    "        self.file_list = file_list\n",
    "        self.label_map = label_map\n",
    "        self.train = train\n",
    "        self.data_len = len(self.file_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.data_len * 3\n",
    "        else:\n",
    "            return self.data_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = None\n",
    "        img_pil = None\n",
    "        if index < self.data_len:\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img_pil = torchvision.transforms.Resize((224,224))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        elif index < 2 * self.data_len:\n",
    "            index = index - self.data_len\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img = torchvision.transforms.RandomHorizontalFlip(p = 1.0)(img)\n",
    "            img_pil = torchvision.transforms.Resize((224,224))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        else:\n",
    "            index = index - 2 * self.data_len\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img = torchvision.transforms.RandomRotation(30)(img)\n",
    "            img_pil = torchvision.transforms.Resize((224,224))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        \n",
    "        if img.shape[0] == 3:\n",
    "            img = torchvision.transforms.Grayscale(num_output_channels=1)(img_pil)\n",
    "            img = torchvision.transforms.ToTensor()(img)\n",
    "        keys = self.file_list[index].split('/')[-1].split('.')[0].split('_')\n",
    "        label = self.label_map[keys[0] + '_' + keys[1]]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WkNa73xzd63",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_img_list, label_map, True)\n",
    "dev_dataset = ImageDataset(val_img_list, label_map)\n",
    "test_dataset = ImageDataset(test_img_list, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1700][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_hist_data(dataset):\n",
    "    dataiter = iter(dataset)\n",
    "    labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataiter.next()\n",
    "        labels += [label]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels_all = [dataset_hist_data(train_dataset), dataset_hist_data(dev_dataset), dataset_hist_data(test_dataset)]\n",
    "n_bins = 30\n",
    "colors = ['red', 'tan', 'lime']\n",
    "plt.hist(labels_all, n_bins, density=True, histtype='bar', color=colors, label=['train', 'dev', 'test'])\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.title(\"Data distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given image filename, return it's corresponding label from label_map\n",
    "def label_util(filename, label_map):\n",
    "    keys = filename.split('/')[-1].split('.')[0].split('_')\n",
    "    label = label_map[keys[0] + '_' + keys[1]]\n",
    "    return label\n",
    "\n",
    "expressions = ['Anger','Contempt','Disgust','Fear','Happy','Sadness','Surprise']\n",
    "idxs = np.random.randint(100, size=8)\n",
    "f, a = plt.subplots(2, 4, figsize=(10, 5))\n",
    "\n",
    "    \n",
    "for i in range(8):\n",
    "    image = io.imread(train_img_list[idxs[i]]) \n",
    "    r, c = i // 4, i % 4\n",
    "    \n",
    "    # Display an image\n",
    "    label_no = label_util(train_img_list[idxs[i]], label_map)\n",
    "    a[r][c].set_title(expressions[label_no])\n",
    "    a[r][c].imshow(image)\n",
    "    a[r][c].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"training_baseline.log\" ,\n",
    "                            filemode=\"a+\")\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQWaXqRtzd65",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset, dev_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, val_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w2FJRhYXzd66",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCO6k3Yfzd69",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, \n",
    "                                               shuffle=True, num_workers=8)\n",
    "\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=32, \n",
    "                                               shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataloader:\n",
    "    print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Wxrwfckzd7B",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "                          nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=kernel_size, stride=stride),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        layers = []\n",
    "        num_classes = 7\n",
    "        channels = [1, 64, 128, 256] # this needs to be modified according to num_blocks\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            layers.append(ConvBlock(C_in=channels[i], C_out=channels[i+1], kernel_size=5, stride=1))\n",
    "        \n",
    "        layers.append(nn.Dropout(p=0.25))\n",
    "        \n",
    "        layers.append(Flatten())\n",
    "        \n",
    "        layers.append(nn.Linear(256*24*24, 512))\n",
    "        \n",
    "        layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        layers.append(nn.Linear(512, num_classes))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6150,
     "status": "ok",
     "timestamp": 1572056426800,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "re2Kdhiazd7C",
    "outputId": "bfb5ad37-62c5-4bf4-e910-452f67ca8b17",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineModel(\n",
      "  (net): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Flatten()\n",
      "    (5): Linear(in_features=147456, out_features=512, bias=True)\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BaselineModel(num_blocks=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRLEOoHdzd7E",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                logger.info('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    " \n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_loss, train_accuracy = test_classify_loss(model,train_dataloader)\n",
    "        test_loss, test_accuracy = test_classify_loss(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        logger.info('Epoch: {}\\tTrain Loss: {}\\tTrain Acc: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, train_loss,train_accuracy, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_loss(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracies = 0\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            accuracies += float(torch.sum(torch.eq(pred_labels, labels)).item())\n",
    "            total+=float(len(labels))\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracies/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30341,
     "status": "error",
     "timestamp": 1572056451006,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "ACkvD4NVzd7G",
    "outputId": "9e3fc8f9-69ae-45f6-de28-8bdbbeb73637",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 23:30:31,010 root         INFO     Epoch: 1\tBatch: 50\tAvg-Loss: 2.7207\n",
      "2019-11-13 23:30:35,193 root         INFO     Epoch: 1\tTrain Loss: 1.8155471994570573\tTrain Acc: 0.2501414827391058\tTest-Loss: 1.7539662633623396\tTest-acc: 0.2449\n",
      "2019-11-13 23:30:40,095 root         INFO     Epoch: 2\tBatch: 50\tAvg-Loss: 1.7589\n",
      "2019-11-13 23:30:44,183 root         INFO     Epoch: 2\tTrain Loss: 1.6762861538958806\tTrain Acc: 0.32371250707413696\tTest-Loss: 1.6753760454606037\tTest-acc: 0.3622\n",
      "2019-11-13 23:30:49,102 root         INFO     Epoch: 3\tBatch: 50\tAvg-Loss: 1.6671\n",
      "2019-11-13 23:30:53,124 root         INFO     Epoch: 3\tTrain Loss: 1.5320388221713737\tTrain Acc: 0.42727787209960383\tTest-Loss: 1.6539227281297957\tTest-acc: 0.4031\n",
      "2019-11-13 23:30:57,992 root         INFO     Epoch: 4\tBatch: 50\tAvg-Loss: 1.4503\n",
      "2019-11-13 23:31:02,100 root         INFO     Epoch: 4\tTrain Loss: 1.1850063110990043\tTrain Acc: 0.5942275042444821\tTest-Loss: 1.468977546205326\tTest-acc: 0.4898\n",
      "2019-11-13 23:31:06,958 root         INFO     Epoch: 5\tBatch: 50\tAvg-Loss: 1.0871\n",
      "2019-11-13 23:31:11,029 root         INFO     Epoch: 5\tTrain Loss: 0.8503697168779832\tTrain Acc: 0.6525183927560838\tTest-Loss: 1.6636184052545198\tTest-acc: 0.4541\n",
      "2019-11-13 23:31:15,898 root         INFO     Epoch: 6\tBatch: 50\tAvg-Loss: 0.9098\n",
      "2019-11-13 23:31:19,979 root         INFO     Epoch: 6\tTrain Loss: 0.6364575609623813\tTrain Acc: 0.7923033389926429\tTest-Loss: 1.382161058941666\tTest-acc: 0.4898\n",
      "2019-11-13 23:31:24,854 root         INFO     Epoch: 7\tBatch: 50\tAvg-Loss: 0.6923\n",
      "2019-11-13 23:31:29,002 root         INFO     Epoch: 7\tTrain Loss: 0.5416215866736659\tTrain Acc: 0.8217317487266553\tTest-Loss: 1.6274151218180755\tTest-acc: 0.5612\n",
      "2019-11-13 23:31:33,819 root         INFO     Epoch: 8\tBatch: 50\tAvg-Loss: 0.6391\n",
      "2019-11-13 23:31:37,974 root         INFO     Epoch: 8\tTrain Loss: 0.4958367603069385\tTrain Acc: 0.8200339558573854\tTest-Loss: 1.9614826708423847\tTest-acc: 0.5153\n",
      "2019-11-13 23:31:42,815 root         INFO     Epoch: 9\tBatch: 50\tAvg-Loss: 0.5520\n",
      "2019-11-13 23:31:46,872 root         INFO     Epoch: 9\tTrain Loss: 0.35903097389779254\tTrain Acc: 0.8862478777589134\tTest-Loss: 1.759834745708777\tTest-acc: 0.5306\n",
      "2019-11-13 23:31:51,667 root         INFO     Epoch: 10\tBatch: 50\tAvg-Loss: 0.4814\n",
      "2019-11-13 23:31:55,840 root         INFO     Epoch: 10\tTrain Loss: 0.376893224204133\tTrain Acc: 0.88907753254103\tTest-Loss: 1.858957558262105\tTest-acc: 0.5255\n",
      "2019-11-13 23:32:00,648 root         INFO     Epoch: 11\tBatch: 50\tAvg-Loss: 0.4683\n",
      "2019-11-13 23:32:04,678 root         INFO     Epoch: 11\tTrain Loss: 0.2700484541424534\tTrain Acc: 0.9117147707979627\tTest-Loss: 2.1127462679026077\tTest-acc: 0.5357\n",
      "2019-11-13 23:32:09,551 root         INFO     Epoch: 12\tBatch: 50\tAvg-Loss: 0.4359\n",
      "2019-11-13 23:32:13,628 root         INFO     Epoch: 12\tTrain Loss: 0.24675882962623524\tTrain Acc: 0.923033389926429\tTest-Loss: 2.538906934309979\tTest-acc: 0.5408\n",
      "2019-11-13 23:32:18,527 root         INFO     Epoch: 13\tBatch: 50\tAvg-Loss: 0.3687\n",
      "2019-11-13 23:32:22,608 root         INFO     Epoch: 13\tTrain Loss: 0.2420020278355599\tTrain Acc: 0.9326542161856254\tTest-Loss: 2.1586826869419644\tTest-acc: 0.5663\n",
      "2019-11-13 23:32:27,462 root         INFO     Epoch: 14\tBatch: 50\tAvg-Loss: 0.3565\n",
      "2019-11-13 23:32:31,587 root         INFO     Epoch: 14\tTrain Loss: 0.20232263904003756\tTrain Acc: 0.9496321448783248\tTest-Loss: 1.7095077816320925\tTest-acc: 0.5357\n",
      "2019-11-13 23:32:36,471 root         INFO     Epoch: 15\tBatch: 50\tAvg-Loss: 0.3238\n",
      "2019-11-13 23:32:40,576 root         INFO     Epoch: 15\tTrain Loss: 0.1679321622099658\tTrain Acc: 0.9501980758347481\tTest-Loss: 2.108149460383824\tTest-acc: 0.5969\n",
      "2019-11-13 23:32:45,459 root         INFO     Epoch: 16\tBatch: 50\tAvg-Loss: 0.3248\n",
      "2019-11-13 23:32:49,561 root         INFO     Epoch: 16\tTrain Loss: 0.16531191185648048\tTrain Acc: 0.9530277306168647\tTest-Loss: 2.3924441775497125\tTest-acc: 0.5408\n",
      "2019-11-13 23:32:54,403 root         INFO     Epoch: 17\tBatch: 50\tAvg-Loss: 0.2940\n",
      "2019-11-13 23:32:58,527 root         INFO     Epoch: 17\tTrain Loss: 0.1429757584765528\tTrain Acc: 0.9598189020939445\tTest-Loss: 2.5308318697676366\tTest-acc: 0.5816\n",
      "2019-11-13 23:33:03,355 root         INFO     Epoch: 18\tBatch: 50\tAvg-Loss: 0.3048\n",
      "2019-11-13 23:33:07,427 root         INFO     Epoch: 18\tTrain Loss: 0.1496664693565374\tTrain Acc: 0.9558573853989814\tTest-Loss: 2.258082696369716\tTest-acc: 0.5204\n",
      "2019-11-13 23:33:12,253 root         INFO     Epoch: 19\tBatch: 50\tAvg-Loss: 0.3021\n",
      "2019-11-13 23:33:16,342 root         INFO     Epoch: 19\tTrain Loss: 0.15379166387519091\tTrain Acc: 0.9434069043576684\tTest-Loss: 2.491970509898906\tTest-acc: 0.6224\n",
      "2019-11-13 23:33:21,200 root         INFO     Epoch: 20\tBatch: 50\tAvg-Loss: 0.2786\n",
      "2019-11-13 23:33:25,278 root         INFO     Epoch: 20\tTrain Loss: 0.12316813021128756\tTrain Acc: 0.9654782116581777\tTest-Loss: 2.2277308191571916\tTest-acc: 0.5918\n",
      "2019-11-13 23:33:30,165 root         INFO     Epoch: 21\tBatch: 50\tAvg-Loss: 0.2597\n",
      "2019-11-13 23:33:34,373 root         INFO     Epoch: 21\tTrain Loss: 0.11854356053392295\tTrain Acc: 0.9711375212224108\tTest-Loss: 2.158930525487783\tTest-acc: 0.5663\n",
      "2019-11-13 23:33:39,201 root         INFO     Epoch: 22\tBatch: 50\tAvg-Loss: 0.2585\n",
      "2019-11-13 23:33:43,368 root         INFO     Epoch: 22\tTrain Loss: 0.13149683018508576\tTrain Acc: 0.9603848330503678\tTest-Loss: 2.2463815942102547\tTest-acc: 0.6122\n",
      "2019-11-13 23:33:48,222 root         INFO     Epoch: 23\tBatch: 50\tAvg-Loss: 0.2561\n",
      "2019-11-13 23:33:52,323 root         INFO     Epoch: 23\tTrain Loss: 0.1056373737453065\tTrain Acc: 0.9722693831352575\tTest-Loss: 2.1010560551468207\tTest-acc: 0.5510\n",
      "2019-11-13 23:33:57,242 root         INFO     Epoch: 24\tBatch: 50\tAvg-Loss: 0.2340\n",
      "2019-11-13 23:34:01,373 root         INFO     Epoch: 24\tTrain Loss: 0.10366835170670559\tTrain Acc: 0.9671760045274477\tTest-Loss: 2.409684896469116\tTest-acc: 0.6020\n",
      "2019-11-13 23:34:06,213 root         INFO     Epoch: 25\tBatch: 50\tAvg-Loss: 0.1952\n",
      "2019-11-13 23:34:10,367 root         INFO     Epoch: 25\tTrain Loss: 0.09758700802711423\tTrain Acc: 0.966044142614601\tTest-Loss: 2.3321211581327477\tTest-acc: 0.5714\n",
      "2019-11-13 23:34:15,234 root         INFO     Epoch: 26\tBatch: 50\tAvg-Loss: 0.1947\n",
      "2019-11-13 23:34:19,375 root         INFO     Epoch: 26\tTrain Loss: 0.08136173689750566\tTrain Acc: 0.9734012450481041\tTest-Loss: 2.324549244374645\tTest-acc: 0.6020\n",
      "2019-11-13 23:34:24,231 root         INFO     Epoch: 27\tBatch: 50\tAvg-Loss: 0.1547\n",
      "2019-11-13 23:34:28,333 root         INFO     Epoch: 27\tTrain Loss: 0.0949472375108687\tTrain Acc: 0.9671760045274477\tTest-Loss: 2.3706028899367975\tTest-acc: 0.6020\n",
      "2019-11-13 23:34:33,186 root         INFO     Epoch: 28\tBatch: 50\tAvg-Loss: 0.2166\n",
      "2019-11-13 23:34:37,290 root         INFO     Epoch: 28\tTrain Loss: 0.11442141542680262\tTrain Acc: 0.976796830786644\tTest-Loss: 1.8551601633733632\tTest-acc: 0.6173\n",
      "2019-11-13 23:34:42,163 root         INFO     Epoch: 29\tBatch: 50\tAvg-Loss: 0.1916\n",
      "2019-11-13 23:34:46,188 root         INFO     Epoch: 29\tTrain Loss: 0.08444558197830608\tTrain Acc: 0.9790605546123373\tTest-Loss: 2.571310685605419\tTest-acc: 0.6122\n",
      "2019-11-13 23:34:51,072 root         INFO     Epoch: 30\tBatch: 50\tAvg-Loss: 0.1849\n",
      "2019-11-13 23:34:55,141 root         INFO     Epoch: 30\tTrain Loss: 0.06041653883597594\tTrain Acc: 0.9841539332201471\tTest-Loss: 2.4014186494204464\tTest-acc: 0.6122\n",
      "2019-11-13 23:34:59,974 root         INFO     Epoch: 31\tBatch: 50\tAvg-Loss: 0.2183\n",
      "2019-11-13 23:35:04,144 root         INFO     Epoch: 31\tTrain Loss: 0.06773224008866773\tTrain Acc: 0.976796830786644\tTest-Loss: 2.6393892570417754\tTest-acc: 0.6122\n",
      "2019-11-13 23:35:08,981 root         INFO     Epoch: 32\tBatch: 50\tAvg-Loss: 0.1888\n",
      "2019-11-13 23:35:13,101 root         INFO     Epoch: 32\tTrain Loss: 0.0925947503963128\tTrain Acc: 0.9717034521788341\tTest-Loss: 2.0102297432568608\tTest-acc: 0.6276\n",
      "2019-11-13 23:35:17,896 root         INFO     Epoch: 33\tBatch: 50\tAvg-Loss: 0.1804\n",
      "2019-11-13 23:35:21,959 root         INFO     Epoch: 33\tTrain Loss: 0.05243851974328224\tTrain Acc: 0.9847198641765704\tTest-Loss: 2.13835680728056\tTest-acc: 0.5867\n",
      "2019-11-13 23:35:26,841 root         INFO     Epoch: 34\tBatch: 50\tAvg-Loss: 0.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 23:35:30,988 root         INFO     Epoch: 34\tTrain Loss: 0.07226855301703514\tTrain Acc: 0.980192416525184\tTest-Loss: 2.5280141003277836\tTest-acc: 0.5867\n",
      "2019-11-13 23:35:35,840 root         INFO     Epoch: 35\tBatch: 50\tAvg-Loss: 0.1394\n",
      "2019-11-13 23:35:39,975 root         INFO     Epoch: 35\tTrain Loss: 0.0563846650466441\tTrain Acc: 0.987549518958687\tTest-Loss: 2.3203429786526426\tTest-acc: 0.6224\n",
      "2019-11-13 23:35:44,892 root         INFO     Epoch: 36\tBatch: 50\tAvg-Loss: 0.1633\n",
      "2019-11-13 23:35:48,915 root         INFO     Epoch: 36\tTrain Loss: 0.05470042634846555\tTrain Acc: 0.9852857951329937\tTest-Loss: 2.605116756594911\tTest-acc: 0.6122\n",
      "2019-11-13 23:35:53,782 root         INFO     Epoch: 37\tBatch: 50\tAvg-Loss: 0.1653\n",
      "2019-11-13 23:35:57,877 root         INFO     Epoch: 37\tTrain Loss: 0.04281981122412787\tTrain Acc: 0.9864176570458404\tTest-Loss: 2.690900607984893\tTest-acc: 0.6276\n",
      "2019-11-13 23:36:02,778 root         INFO     Epoch: 38\tBatch: 50\tAvg-Loss: 0.1655\n",
      "2019-11-13 23:36:06,927 root         INFO     Epoch: 38\tTrain Loss: 0.051883247931317614\tTrain Acc: 0.9841539332201471\tTest-Loss: 2.2680246002820073\tTest-acc: 0.6582\n",
      "2019-11-13 23:36:11,751 root         INFO     Epoch: 39\tBatch: 50\tAvg-Loss: 0.1357\n",
      "2019-11-13 23:36:15,909 root         INFO     Epoch: 39\tTrain Loss: 0.05953531293646471\tTrain Acc: 0.989247311827957\tTest-Loss: 1.9121863939324204\tTest-acc: 0.6071\n",
      "2019-11-13 23:36:20,749 root         INFO     Epoch: 40\tBatch: 50\tAvg-Loss: 0.1383\n",
      "2019-11-13 23:36:24,874 root         INFO     Epoch: 40\tTrain Loss: 0.05613855885191023\tTrain Acc: 0.9835880022637238\tTest-Loss: 2.285120219600444\tTest-acc: 0.5816\n",
      "2019-11-13 23:36:29,762 root         INFO     Epoch: 41\tBatch: 50\tAvg-Loss: 0.1573\n",
      "2019-11-13 23:36:33,850 root         INFO     Epoch: 41\tTrain Loss: 0.06779117306381312\tTrain Acc: 0.9796264855687606\tTest-Loss: 2.364191464015416\tTest-acc: 0.5714\n",
      "2019-11-13 23:36:38,681 root         INFO     Epoch: 42\tBatch: 50\tAvg-Loss: 0.1398\n",
      "2019-11-13 23:36:42,753 root         INFO     Epoch: 42\tTrain Loss: 0.04510470763709023\tTrain Acc: 0.9858517260894171\tTest-Loss: 2.6317324936389923\tTest-acc: 0.6429\n",
      "2019-11-13 23:36:47,578 root         INFO     Epoch: 43\tBatch: 50\tAvg-Loss: 0.1258\n",
      "2019-11-13 23:36:51,608 root         INFO     Epoch: 43\tTrain Loss: 0.05470738431357636\tTrain Acc: 0.9813242784380306\tTest-Loss: 2.846866359516066\tTest-acc: 0.5969\n",
      "2019-11-13 23:36:56,430 root         INFO     Epoch: 44\tBatch: 50\tAvg-Loss: 0.1442\n",
      "2019-11-13 23:37:00,578 root         INFO     Epoch: 44\tTrain Loss: 0.05968584748376087\tTrain Acc: 0.9824561403508771\tTest-Loss: 2.580823915345328\tTest-acc: 0.6173\n",
      "2019-11-13 23:37:05,404 root         INFO     Epoch: 45\tBatch: 50\tAvg-Loss: 0.1046\n",
      "2019-11-13 23:37:09,488 root         INFO     Epoch: 45\tTrain Loss: 0.038176678100205017\tTrain Acc: 0.989247311827957\tTest-Loss: 2.35191696030753\tTest-acc: 0.6531\n",
      "2019-11-13 23:37:14,384 root         INFO     Epoch: 46\tBatch: 50\tAvg-Loss: 0.1206\n",
      "2019-11-13 23:37:18,454 root         INFO     Epoch: 46\tTrain Loss: 0.05209756093161643\tTrain Acc: 0.9835880022637238\tTest-Loss: 2.626965513034743\tTest-acc: 0.6122\n",
      "2019-11-13 23:37:23,316 root         INFO     Epoch: 47\tBatch: 50\tAvg-Loss: 0.1266\n",
      "2019-11-13 23:37:27,436 root         INFO     Epoch: 47\tTrain Loss: 0.028585760893714448\tTrain Acc: 0.9926428975664969\tTest-Loss: 2.6760018504395777\tTest-acc: 0.6684\n",
      "2019-11-13 23:37:32,355 root         INFO     Epoch: 48\tBatch: 50\tAvg-Loss: 0.1260\n",
      "2019-11-13 23:37:36,399 root         INFO     Epoch: 48\tTrain Loss: 0.03729140495430271\tTrain Acc: 0.987549518958687\tTest-Loss: 2.478656846649793\tTest-acc: 0.6633\n",
      "2019-11-13 23:37:41,257 root         INFO     Epoch: 49\tBatch: 50\tAvg-Loss: 0.1193\n",
      "2019-11-13 23:37:45,321 root         INFO     Epoch: 49\tTrain Loss: 0.0314823293918243\tTrain Acc: 0.9932088285229203\tTest-Loss: 2.3587738543140646\tTest-acc: 0.6429\n",
      "2019-11-13 23:37:50,147 root         INFO     Epoch: 50\tBatch: 50\tAvg-Loss: 0.1078\n",
      "2019-11-13 23:37:54,168 root         INFO     Epoch: 50\tTrain Loss: 0.030612743298986153\tTrain Acc: 0.9915110356536503\tTest-Loss: 2.619712055945883\tTest-acc: 0.6582\n",
      "2019-11-13 23:37:58,960 root         INFO     Epoch: 51\tBatch: 50\tAvg-Loss: 0.1259\n",
      "2019-11-13 23:38:03,071 root         INFO     Epoch: 51\tTrain Loss: 0.04443675060200452\tTrain Acc: 0.9881154499151104\tTest-Loss: 2.1893809912156086\tTest-acc: 0.6173\n",
      "2019-11-13 23:38:07,921 root         INFO     Epoch: 52\tBatch: 50\tAvg-Loss: 0.1249\n",
      "2019-11-13 23:38:11,999 root         INFO     Epoch: 52\tTrain Loss: 0.04567560342738187\tTrain Acc: 0.9898132427843803\tTest-Loss: 2.2386540271797957\tTest-acc: 0.6378\n",
      "2019-11-13 23:38:16,922 root         INFO     Epoch: 53\tBatch: 50\tAvg-Loss: 0.1113\n",
      "2019-11-13 23:38:21,025 root         INFO     Epoch: 53\tTrain Loss: 0.03493088291538201\tTrain Acc: 0.989247311827957\tTest-Loss: 3.246218759186414\tTest-acc: 0.6378\n",
      "2019-11-13 23:38:25,870 root         INFO     Epoch: 54\tBatch: 50\tAvg-Loss: 0.1677\n",
      "2019-11-13 23:38:29,988 root         INFO     Epoch: 54\tTrain Loss: 0.04226567363867196\tTrain Acc: 0.9852857951329937\tTest-Loss: 2.973912253671763\tTest-acc: 0.6684\n",
      "2019-11-13 23:38:34,878 root         INFO     Epoch: 55\tBatch: 50\tAvg-Loss: 0.1367\n",
      "2019-11-13 23:38:39,043 root         INFO     Epoch: 55\tTrain Loss: 0.029178861311501013\tTrain Acc: 0.9920769666100736\tTest-Loss: 2.5396366265355326\tTest-acc: 0.6480\n",
      "2019-11-13 23:38:43,943 root         INFO     Epoch: 56\tBatch: 50\tAvg-Loss: 0.1129\n",
      "2019-11-13 23:38:47,988 root         INFO     Epoch: 56\tTrain Loss: 0.03657804199307775\tTrain Acc: 0.987549518958687\tTest-Loss: 2.567145298938362\tTest-acc: 0.6224\n",
      "2019-11-13 23:38:52,842 root         INFO     Epoch: 57\tBatch: 50\tAvg-Loss: 0.1290\n",
      "2019-11-13 23:38:56,944 root         INFO     Epoch: 57\tTrain Loss: 0.031213451798817637\tTrain Acc: 0.9886813808715337\tTest-Loss: 2.6692678101208744\tTest-acc: 0.6735\n",
      "2019-11-13 23:39:01,804 root         INFO     Epoch: 58\tBatch: 50\tAvg-Loss: 0.1198\n",
      "2019-11-13 23:39:05,866 root         INFO     Epoch: 58\tTrain Loss: 0.04343166672026429\tTrain Acc: 0.9852857951329937\tTest-Loss: 3.0782793687314403\tTest-acc: 0.5918\n",
      "2019-11-13 23:39:10,726 root         INFO     Epoch: 59\tBatch: 50\tAvg-Loss: 0.1044\n",
      "2019-11-13 23:39:14,919 root         INFO     Epoch: 59\tTrain Loss: 0.016863209416274977\tTrain Acc: 0.9966044142614601\tTest-Loss: 2.520918286576563\tTest-acc: 0.6378\n",
      "2019-11-13 23:39:22,369 root         INFO     Epoch: 60\tBatch: 50\tAvg-Loss: 0.1262\n",
      "2019-11-13 23:39:27,842 root         INFO     Epoch: 60\tTrain Loss: 0.023931004331527825\tTrain Acc: 0.9937747594793436\tTest-Loss: 2.6013223589683068\tTest-acc: 0.6224\n",
      "2019-11-13 23:39:36,757 root         INFO     Epoch: 61\tBatch: 50\tAvg-Loss: 0.1087\n"
     ]
    }
   ],
   "source": [
    "train_losses, eval_losses, eval_accs = train(model,200, train_dataloader,dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6AXVsznzd7J",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses)\n",
    "plt.savefig(\"training_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(eval_accs)\n",
    "plt.savefig(\"val_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_cnn (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
