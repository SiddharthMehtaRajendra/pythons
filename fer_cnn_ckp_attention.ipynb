{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdRpCepJz7XV",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import centerloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34BLnzCbzd6w",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "#981\n",
    "train_size= 589\n",
    "val_size= 196\n",
    "test_size = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXc2tVVQ7Cds",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_data(datadir, label_map):\n",
    "    img_list = []\n",
    "    file_list = []\n",
    "    \n",
    "    for root, directories, filenames in os.walk(datadir):      \n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.png'):\n",
    "                \n",
    "                filei = os.path.join(root, filename)\n",
    "                file_ids = filename.split('_')\n",
    "                file_id = file_ids[0] + '_' + file_ids[1]\n",
    "                if file_id in label_map:\n",
    "                    img_list.append(filei)\n",
    "    return img_list[:train_size], img_list[train_size:train_size+val_size], img_list[train_size+val_size: train_size+val_size+test_size]\n",
    "#     return img_list\n",
    "\n",
    "\n",
    "\n",
    "def parse_emotion_data(datadir):\n",
    "    em_map = {}\n",
    "    file_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):\n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.txt'):\n",
    "                   \n",
    "                f = open(root +  \"/\" + filename, 'r')\n",
    "                lines = []\n",
    "                for line in f.readlines():\n",
    "                    lines.append(line)\n",
    "                value = lines[0]\n",
    "                f.close()\n",
    "                \n",
    "                keys = filename.split('_')\n",
    "                key = keys[0] + '_' + keys[1]\n",
    "                em_map[key] = int(float(value.strip())) - 1\n",
    "                \n",
    "    return em_map\n",
    "\n",
    "\n",
    "def split_folds_data(data, fold_id, num_folds):\n",
    "    fold_size = len(data)//num_folds\n",
    "    print(fold_size)\n",
    "    data = data[:fold_size * num_folds]\n",
    "\n",
    "    \n",
    "    val_fold_ids = [(fold_id) % num_folds, (fold_id + 1) % num_folds]\n",
    "    test_fold_ids = [(fold_id + 2) % num_folds, (fold_id + 3) % num_folds]\n",
    "    \n",
    "    data_div = [data[i*fold_size : (i+1)*fold_size] for i in range(num_folds)]\n",
    "    \n",
    "    t_data = np.concatenate([data_div[i] for i in range(num_folds) if i not in val_fold_ids and i not in test_fold_ids], axis=0)\n",
    "    \n",
    "    v_data = np.concatenate([data_div[fid] for fid in val_fold_ids], axis = 0)\n",
    "    \n",
    "    test_data = np.concatenate([data_div[fid] for fid in test_fold_ids], axis = 0)\n",
    "    return t_data, v_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "label_map = parse_emotion_data(\"Emotion\")\n",
    "# img_list = parse_data(\"cohn-kanade-images\", label_map)\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     t_data, v_data, test_data = split_folds_data(img_list, i, 10)\n",
    "#     print(\"img_list\", len(img_list))\n",
    "#     print(\"t_data len\", len(t_data))\n",
    "#     print(\"v_data len\", len(v_data))\n",
    "#     print(\"test_data len\", len(test_data))\n",
    "#     print(\"\\n\");\n",
    "    \n",
    "\n",
    "#     assert 0.6 * (len(img_list) - 1) == len(t_data)\n",
    "#     assert 0.2 * (len(img_list) - 1) == len(v_data)\n",
    "#     assert 0.2 * (len(img_list) - 1) == len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "config = Config(\n",
    "    num_classes = 7,\n",
    "    width = 224,\n",
    "    height = 224,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 32,\n",
    "    feat_dim = 7,\n",
    "    lr_cent = 0.5,\n",
    "    closs_weight = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1572058491942,
     "user": {
      "displayName": "Mohit Grover",
      "photoUrl": "",
      "userId": "17020987855024013058"
     },
     "user_tz": 420
    },
    "id": "tTjycRTo9_KN",
    "outputId": "95d0dd64-73dd-47cc-8d4e-993b43bd77d0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, label_map, train = False):\n",
    "        self.file_list = file_list\n",
    "        self.label_map = label_map\n",
    "        self.train = train\n",
    "        self.data_len = len(self.file_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.data_len * 5\n",
    "        else:\n",
    "            return self.data_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = None\n",
    "        img_pil = None\n",
    "        img_h = config.width\n",
    "        img_w = config.height\n",
    "        if index < self.data_len:\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img_pil = torchvision.transforms.Resize((img_h,img_w))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        elif index < 2 * self.data_len:\n",
    "            index = index - self.data_len\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img = torchvision.transforms.RandomHorizontalFlip(p = 1.0)(img)\n",
    "            img_pil = torchvision.transforms.Resize((img_h,img_w))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        elif index < 3 * self.data_len:\n",
    "            index = index - 2 * self.data_len\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img = torchvision.transforms.RandomRotation(30)(img)\n",
    "            img_pil = torchvision.transforms.Resize((img_h,img_w))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        elif index < 4 * self.data_len:\n",
    "            index = index - 3 * self.data_len\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img = torchvision.transforms.RandomAffine(5, translate=(0.1,0.1), scale=(1.1,1.2), shear=0, resample=False, fillcolor=0)(img)\n",
    "            img_pil = torchvision.transforms.Resize((img_h,img_w))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        else:\n",
    "            index = index - 4 * self.data_len\n",
    "            img = Image.open(self.file_list[index])\n",
    "            img = torchvision.transforms.RandomAffine(5, translate=(0,0), scale=(1.1,1.2), shear=5, resample=False, fillcolor=0)(img)\n",
    "            img_pil = torchvision.transforms.Resize((img_h,img_w))(img)\n",
    "            img = torchvision.transforms.ToTensor()(img_pil)\n",
    "            \n",
    "            \n",
    "        \n",
    "        if img.shape[0] == 3:\n",
    "            img = torchvision.transforms.Grayscale(num_output_channels=1)(img_pil)\n",
    "            img = torchvision.transforms.ToTensor()(img)\n",
    "        img = torchvision.transforms.Normalize(mean=[0.485], std=[0.229])(img)\n",
    "        keys = self.file_list[index].split('/')[-1].split('.')[0].split('_')\n",
    "        label = self.label_map[keys[0] + '_' + keys[1]]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_hist_data(dataset):\n",
    "    dataiter = iter(dataset)\n",
    "    labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataiter.next()\n",
    "        labels += [label]\n",
    "    return labels\n",
    "\n",
    "\n",
    "# labels_all = [dataset_hist_data(train_dataset), dataset_hist_data(dev_dataset), dataset_hist_data(test_dataset)]\n",
    "# n_bins = 30\n",
    "# colors = ['red', 'tan', 'lime']\n",
    "# plt.hist(labels_all, n_bins, density=True, histtype='bar', color=colors, label=['train', 'dev', 'test'])\n",
    "# plt.legend(prop={'size': 10})\n",
    "# plt.title(\"Data distribution\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given image filename, return it's corresponding label from label_map\n",
    "def label_util(filename, label_map):\n",
    "    keys = filename.split('/')[-1].split('.')[0].split('_')\n",
    "    label = label_map[keys[0] + '_' + keys[1]]\n",
    "    return label\n",
    "\n",
    "# expressions = ['Anger','Contempt','Disgust','Fear','Happy','Sadness','Surprise']\n",
    "# idxs = np.random.randint(100, size=8)\n",
    "# f, a = plt.subplots(2, 4, figsize=(10, 5))\n",
    "\n",
    "    \n",
    "# for i in range(8):\n",
    "#     image = io.imread(train_img_list[idxs[i]]) \n",
    "#     r, c = i // 4, i % 4\n",
    "    \n",
    "#     # Display an image\n",
    "#     label_no = label_util(train_img_list[idxs[i]], label_map)\n",
    "#     a[r][c].set_title(expressions[label_no])\n",
    "#     a[r][c].imshow(image)\n",
    "#     a[r][c].axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"training_fer_ckp.log\" ,\n",
    "                            filemode=\"a+\")\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQWaXqRtzd65",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset, dev_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, val_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w2FJRhYXzd66",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCO6k3Yfzd69",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Wxrwfckzd7B",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input1_size, input2_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_fclayer = nn.Linear(input2_size, input1_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        \n",
    "        self.input1 = input1\n",
    "        self.input2 = input2\n",
    "        \n",
    "        self.batch, self.outchannel1, self.h1, self.w1 = input1.shape\n",
    "        self.batch, self.outchannel2 = input2.shape\n",
    "        \n",
    "        input2_rescaled = self.attention_fclayer(self.input2)\n",
    "        input2_rescaled = self.input2\n",
    "        \n",
    "        input2_bmm = input2_rescaled.view(self.batch,self.outchannel1,1)\n",
    "        \n",
    "        compat_scores = torch.zeros((self.batch, self.h1*self.w1))\n",
    "        compat_scores = compat_scores.to(device)\n",
    "\n",
    "        for h in range(self.h1):\n",
    "            for w in range(self.w1):\n",
    "                input1_bmm = self.input1[:,:,h,w].view(self.batch,1,self.outchannel1)\n",
    "                compat_scores[:,h*self.w1+w] = torch.bmm(input1_bmm, input2_bmm).squeeze()\n",
    "        \n",
    "        normalized_compat_scores = F.softmax(compat_scores, dim=1)\n",
    "        \n",
    "        bmm_arg2 = self.input1.view(self.batch,self.outchannel1,self.h1*self.w1,1)\n",
    "        bmm_argtemp = normalized_compat_scores.view(self.batch,1,self.h1*self.w1).repeat(1,self.outchannel1,1)\n",
    "        bmm_arg1 = bmm_argtemp.view(self.batch,self.outchannel1,1,self.h1*self.w1)\n",
    "        \n",
    "        g_mod = torch.zeros((self.batch, self.outchannel1))\n",
    "        g_mod = g_mod.to(device)\n",
    "        \n",
    "        for b in range(self.batch):\n",
    "            g_mod[b,:] = torch.bmm(bmm_arg1[b,:,:,:], bmm_arg2[b,:,:,:]).squeeze()\n",
    "        \n",
    "        return g_mod\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "                          nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=kernel_size, stride=stride, padding=(1,1)),\n",
    "                          nn.BatchNorm2d(C_out),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.linblock = nn.Sequential(\n",
    "                          nn.Linear(insize, outsize),\n",
    "                          nn.BatchNorm1d(outsize),\n",
    "                          nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linblock(x)\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        layers = []\n",
    "        num_classes = 7\n",
    "        channels = [1, 256, 128, 64] # this needs to be modified according to num_blocks\n",
    "        linear_size = [64*28*28, 512, 256]\n",
    "        \n",
    "        self.convlayer1 = ConvBlock(C_in=channels[0], C_out=channels[1], kernel_size=3, stride=1)\n",
    "        self.convlayer2 = ConvBlock(C_in=channels[1], C_out=channels[2], kernel_size=3, stride=1)\n",
    "        self.convlayer3 = ConvBlock(C_in=channels[2], C_out=channels[3], kernel_size=3, stride=1)        \n",
    "        \n",
    "        self.flattenlayer = Flatten()\n",
    "        \n",
    "        self.fclayer1 = nn.Linear(linear_size[0], linear_size[1])\n",
    "        self.fclayer2 = nn.Linear(linear_size[1], linear_size[2])\n",
    "        \n",
    "        self.attlayer1 = AttentionLayer(channels[1],linear_size[2])\n",
    "        \n",
    "        self.fclayer3 = nn.Linear(channels[1], config.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.out1 = self.convlayer1(x)\n",
    "        self.out2 = self.convlayer2(self.out1)\n",
    "        self.out3 = self.convlayer3(self.out2)\n",
    "        \n",
    "        self.out4 = self.flattenlayer(self.out3)\n",
    "        \n",
    "        self.out5 = self.fclayer1(self.out4)\n",
    "        self.out6 = self.fclayer2(self.out5)\n",
    "        \n",
    "        self.attout = self.attlayer1(self.out1, self.out6)\n",
    "        \n",
    "        self.out = self.fclayer3(self.attout)\n",
    "        \n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6150,
     "status": "ok",
     "timestamp": 1572056426800,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "re2Kdhiazd7C",
    "outputId": "bfb5ad37-62c5-4bf4-e910-452f67ca8b17",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "criterion_closs = centerloss.CenterLoss(config.num_classes, config.feat_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_closs(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            outputs = model(feats)\n",
    "            \n",
    "            loss = criterion(outputs, labels.long())\n",
    "            c_loss = criterion_closs(outputs, labels.long())\n",
    "            loss = loss + config.closs_weight * c_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / config.closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                logger.info('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    " \n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_loss, train_accuracy = test_classify_closs(model,train_dataloader)\n",
    "        test_loss, test_accuracy = test_classify_closs(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        logger.info('Epoch: {}\\tTrain Loss: {}\\tTrain Acc: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, train_loss,train_accuracy, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_closs(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracies = 0\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            c_loss = criterion_closs(outputs, labels.long())\n",
    "            loss = loss + config.closs_weight * c_loss\n",
    "            \n",
    "            accuracies += float(torch.sum(torch.eq(pred_labels, labels)).item())\n",
    "            total+=float(len(labels))\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracies/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRLEOoHdzd7E",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                logger.info('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    " \n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_loss, train_accuracy = test_classify_loss(model,train_dataloader)\n",
    "        test_loss, test_accuracy = test_classify_loss(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        logger.info('Epoch: {}\\tTrain Loss: {}\\tTrain Acc: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, train_loss,train_accuracy, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_loss(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracies = 0\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            accuracies += float(torch.sum(torch.eq(pred_labels, labels)).item())\n",
    "            total+=float(len(labels))\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracies/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30341,
     "status": "error",
     "timestamp": 1572056451006,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "ACkvD4NVzd7G",
    "outputId": "9e3fc8f9-69ae-45f6-de28-8bdbbeb73637",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AttentionLayer' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91fc8ec9312d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     train_img_list, val_img_list, test_list = split_folds_data(img_list, fold_id, num_folds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaselineModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f1d31573d8ea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_blocks)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfclayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinear_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfclayer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f1d31573d8ea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input1_size, input2_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_fclayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput2_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutchannel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    589\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 591\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AttentionLayer' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "num_folds = 1\n",
    "running_val_acc = 0.0\n",
    "running_test_acc = 0.0\n",
    "batch_size = config.batch_size\n",
    "num_epochs = config.num_epochs\n",
    "for fold_id in range(0, num_folds):\n",
    "    \n",
    "#     train_img_list, val_img_list, test_list = split_folds_data(img_list, fold_id, num_folds)\n",
    "    model = BaselineModel(num_blocks=3)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    #optimizer_closs = optim.Adam(model.parameters())\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    train_img_list, val_img_list, test_img_list = parse_data(\"cohn-kanade-images\", label_map)\n",
    "    train_dataset = ImageDataset(train_img_list, label_map, train = True)\n",
    "    dev_dataset = ImageDataset(val_img_list, label_map)\n",
    "    test_dataset = ImageDataset(test_img_list, label_map)\n",
    "\n",
    "    # dataset based on fold_id\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                               shuffle=True, num_workers=8,drop_last=True)\n",
    "\n",
    "    dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, \n",
    "                                               shuffle=True, num_workers=8, drop_last=True)\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                               shuffle=False, num_workers=8, drop_last=True)\n",
    "\n",
    "    train_losses, eval_losses, eval_accs = train(model, num_epochs, train_dataloader,dev_dataloader)\n",
    "    running_val_acc += eval_accs[-1]\n",
    "    \n",
    "    test_loss, test_acc = test_classify_loss(model, test_dataloader)\n",
    "    running_test_acc += test_acc\n",
    "    \n",
    "    \n",
    "final_val_acc = running_val_acc / num_folds\n",
    "final_test_acc = running_test_acc / num_folds\n",
    "\n",
    "logger.info(\"val acc | test acc {} {} \".format(final_val_acc, final_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"models/fer_cnn_ckp_augmentation_adam_closs_200_epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(test_img_list, label_map)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, \n",
    "                                               shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test_classify_loss(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6AXVsznzd7J",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses)\n",
    "plt.savefig(\"training_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(eval_accs)\n",
    "plt.savefig(\"val_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_cnn (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
