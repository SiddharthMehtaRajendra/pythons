{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "99mWUL-r7tLB"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hdRpCepJz7XV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "34BLnzCbzd6w"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CXc2tVVQ7Cds"
   },
   "outputs": [],
   "source": [
    "def parse_data(datadir, label_map):\n",
    "    img_list = []\n",
    "    file_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):      \n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.png'):\n",
    "                \n",
    "                filei = os.path.join(root, filename)\n",
    "                file_id = filename.split('.')[0]\n",
    "                \n",
    "                if file_id in label_map:\n",
    "                    img_list.append(filei)\n",
    "    \n",
    "    return img_list\n",
    "\n",
    "\n",
    "\n",
    "def parse_emotion_data(datadir):\n",
    "    em_map = {}\n",
    "    file_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):\n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.txt'):\n",
    "                   \n",
    "                f = open(root +  \"/\" + filename, 'r')\n",
    "                lines = []\n",
    "                for line in f.readlines():\n",
    "                    lines.append(line)\n",
    "                value = lines[0]\n",
    "                f.close()\n",
    "                \n",
    "                key = filename.split('_e')[0]\n",
    "                em_map[key] = int(float(value.strip()))\n",
    "                \n",
    "    return em_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2602,
     "status": "ok",
     "timestamp": 1572058901817,
     "user": {
      "displayName": "Mohit Grover",
      "photoUrl": "",
      "userId": "17020987855024013058"
     },
     "user_tz": 420
    },
    "id": "GPYh7Um17Wey",
    "outputId": "c9399d33-1f2e-49d4-a696-e7c002299dd5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLHRJaoSFLkmN+F9MM9jydRWpMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c0de110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_map = parse_emotion_data(\"Emotion\")\n",
    "img_list = parse_data(\"cohn-kanade-images\", label_map)\n",
    "\n",
    "freq_count = collections.Counter(label_map.values())\n",
    "counts = [freq_count[key] for key in sorted(freq_count.keys())]\n",
    "\n",
    "\n",
    "bins = 30\n",
    "plt.hist(label_map.values(), bins, facecolor='blue', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1572058491942,
     "user": {
      "displayName": "Mohit Grover",
      "photoUrl": "",
      "userId": "17020987855024013058"
     },
     "user_tz": 420
    },
    "id": "tTjycRTo9_KN",
    "outputId": "95d0dd64-73dd-47cc-8d4e-993b43bd77d0"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, label_map):\n",
    "        self.file_list = file_list\n",
    "#         self.n_class = len(label_map)\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img_pil = torchvision.transforms.Resize((224,224))(img)\n",
    "        img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        if img.shape[0] == 3:\n",
    "            img = torchvision.transforms.Grayscale(num_output_channels=1)(img_pil)\n",
    "            img = torchvision.transforms.ToTensor()(img)\n",
    "        label = self.label_map[self.file_list[index].split('/')[-1].split('.')[0]]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8WkNa73xzd63"
   },
   "outputs": [],
   "source": [
    "dataset = ImageDataset(img_list, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DQWaXqRtzd65"
   },
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset, test_dataset = torch.utils.data.random_split(dataset, (195, 66, 66))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "w2FJRhYXzd66"
   },
   "outputs": [],
   "source": [
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cCO6k3Yfzd69"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, \n",
    "                                               shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Wj46-XYLzd6_"
   },
   "outputs": [],
   "source": [
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=64, \n",
    "                                               shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input1_size, input2_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_fclayer = nn.Linear(input2_size, input1_size)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        \n",
    "        self.input1 = input1\n",
    "        self.input2 = input2\n",
    "        \n",
    "        self.batch, self.outchannel1, self.h1, self.w1 = input1.shape\n",
    "        self.batch, self.outchannel2 = input2.shape\n",
    "        \n",
    "        input2_rescaled = self.attention_fclayer(self.input2)\n",
    "        \n",
    "        input2_bmm = input2_rescaled.view(self.batch,self.outchannel1,1)\n",
    "        \n",
    "        compat_scores = torch.zeros((self.batch, self.h1*self.w1))\n",
    "\n",
    "        for h in range(self.h1):\n",
    "            for w in range(self.w1):\n",
    "                input1_bmm = self.input1[:,:,h,w].view(self.batch,1,self.outchannel1)\n",
    "                compat_scores[:,h*self.w1+w] = torch.bmm(input1_bmm, input2_bmm).squeeze()\n",
    "        \n",
    "        normalized_compat_scores = F.softmax(compat_scores)\n",
    "        \n",
    "        bmm_arg2 = self.input1.view(self.batch,self.outchannel1,self.h1*self.w1,1)\n",
    "        bmm_argtemp = normalized_compat_scores.view(self.batch,1,self.h1*self.w1).repeat(1,self.outchannel1,1)\n",
    "        bmm_arg1 = bmm_argtemp.view(self.batch,self.outchannel1,1,self.h1*self.w1)\n",
    "        \n",
    "        g_mod = torch.zeros((self.batch, self.outchannel1))\n",
    "        \n",
    "        for b in range(self.batch):\n",
    "            g_mod[b,:] = torch.bmm(bmm_arg1[b,:,:,:], bmm_arg2[b,:,:,:]).squeeze()\n",
    "        \n",
    "        return g_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1Wxrwfckzd7B"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "                          nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=kernel_size, stride=stride),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        layers = []\n",
    "        self.num_classes = 13\n",
    "        channels = [1, 64, 128, 256] # this needs to be modified according to num_blocks\n",
    "        \n",
    "        self.convlayer1 = ConvBlock(C_in=channels[0], C_out=channels[1], kernel_size=5, stride=1)\n",
    "        self.convlayer2 = ConvBlock(C_in=channels[1], C_out=channels[2], kernel_size=5, stride=1)\n",
    "        self.convlayer3 = ConvBlock(C_in=channels[2], C_out=channels[3], kernel_size=5, stride=1)\n",
    "        \n",
    "#         for i in range(num_blocks):\n",
    "#             layers.append(ConvBlock(C_in=channels[i], C_out=channels[i+1], kernel_size=5, stride=1))\n",
    "        \n",
    "        self.dropoutlayer1 = nn.Dropout(p=0.25)\n",
    "        \n",
    "#         layers.append(nn.Dropout(p=0.25))\n",
    "        \n",
    "        self.flattenlayer = Flatten()\n",
    "        \n",
    "#         layers.append(Flatten())\n",
    "        \n",
    "        self.fclayer1 = nn.Linear(256*24*24, 512)\n",
    "        \n",
    "#         layers.append(nn.Linear(256*24*24, 512))\n",
    "        \n",
    "        self.dropoutlayer2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.attlayer1 = AttentionLayer(channels[1],512)\n",
    "        self.attlayer2 = AttentionLayer(channels[2],512)\n",
    "        self.attlayer3 = AttentionLayer(channels[3],512)\n",
    "        \n",
    "#         layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "#         fclayer2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "#         layers.append(nn.Linear(512, num_classes))\n",
    "        \n",
    "#         self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out1 = self.convlayer1(x)\n",
    "        self.out2 = self.convlayer2(self.out1)\n",
    "        self.out3 = self.convlayer3(self.out2)\n",
    "        self.out4 = self.dropoutlayer1(self.out3)\n",
    "        self.out5 = self.flattenlayer(self.out4)\n",
    "        self.out6 = self.fclayer1(self.out5)\n",
    "        self.out7 = self.dropoutlayer2(self.out6)\n",
    "        \n",
    "        self.attout1 = self.attlayer1(self.out1,self.out7)\n",
    "        self.attout2 = self.attlayer2(self.out2,self.out7)\n",
    "        self.attout3 = self.attlayer3(self.out3,self.out7)\n",
    "        \n",
    "        self.attout = torch.cat((self.attout1,self.attout2,self.attout3),1)\n",
    "        \n",
    "        _, self.att_size = self.attout.shape\n",
    "        \n",
    "        self.fclayer2 = nn.Linear(self.att_size, self.num_classes)\n",
    "        self.final_out = self.fclayer2(self.attout)\n",
    "        \n",
    "        return self.attout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6150,
     "status": "ok",
     "timestamp": 1572056426800,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "re2Kdhiazd7C",
    "outputId": "bfb5ad37-62c5-4bf4-e910-452f67ca8b17"
   },
   "outputs": [],
   "source": [
    "model = BaselineModel(num_blocks=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "# print(model)\n",
    "# print(model(dataset[0][0].unsqueeze(dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kRLEOoHdzd7E"
   },
   "outputs": [],
   "source": [
    "def train(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                logging.debug('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_losses.append(avg_loss)\n",
    "        test_loss, test_accuracy = test_classify_loss(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        print('Epoch: {}\\tTrain Loss: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, avg_loss, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_loss(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracy = 0\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30341,
     "status": "error",
     "timestamp": 1572056451006,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "ACkvD4NVzd7G",
    "outputId": "9e3fc8f9-69ae-45f6-de28-8bdbbeb73637"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumeet/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "train_losses, eval_losses, eval_accs = train(model,200, train_dataloader,dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6AXVsznzd7J"
   },
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses)\n",
    "plt.savefig(\"training_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(eval_accs)\n",
    "plt.savefig(\"val_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(1,2,3,4)\n",
    "print(x.view(1,2,12,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_cnn (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
