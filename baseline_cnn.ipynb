{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "99mWUL-r7tLB"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hdRpCepJz7XV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "34BLnzCbzd6w"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CXc2tVVQ7Cds"
   },
   "outputs": [],
   "source": [
    "def parse_data(datadir, label_map):\n",
    "    img_list = []\n",
    "    file_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):      \n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.png'):\n",
    "                \n",
    "                filei = os.path.join(root, filename)\n",
    "                file_id = filename.split('.')[0]\n",
    "                \n",
    "                if file_id in label_map:\n",
    "                    img_list.append(filei)\n",
    "    \n",
    "    return img_list\n",
    "\n",
    "\n",
    "\n",
    "def parse_emotion_data(datadir):\n",
    "    em_map = {}\n",
    "    file_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):\n",
    "        for filename in filenames:\n",
    "            file_list.append(filename)\n",
    "            if filename.endswith('.txt'):\n",
    "                   \n",
    "                f = open(root +  \"/\" + filename, 'r')\n",
    "                lines = []\n",
    "                for line in f.readlines():\n",
    "                    lines.append(line)\n",
    "                value = lines[0]\n",
    "                f.close()\n",
    "                \n",
    "                key = filename.split('_e')[0]\n",
    "                em_map[key] = int(float(value.strip())) - 1\n",
    "                \n",
    "    return em_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2602,
     "status": "ok",
     "timestamp": 1572058901817,
     "user": {
      "displayName": "Mohit Grover",
      "photoUrl": "",
      "userId": "17020987855024013058"
     },
     "user_tz": 420
    },
    "id": "GPYh7Um17Wey",
    "outputId": "c9399d33-1f2e-49d4-a696-e7c002299dd5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120ba1d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_map = parse_emotion_data(\"Emotion\")\n",
    "img_list = parse_data(\"cohn-kanade-images\", label_map)\n",
    "\n",
    "freq_count = collections.Counter(label_map.values())\n",
    "counts = [freq_count[key] for key in sorted(freq_count.keys())]\n",
    "\n",
    "\n",
    "bins = 30\n",
    "plt.hist(label_map.values(), bins, facecolor='blue', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1572058491942,
     "user": {
      "displayName": "Mohit Grover",
      "photoUrl": "",
      "userId": "17020987855024013058"
     },
     "user_tz": 420
    },
    "id": "tTjycRTo9_KN",
    "outputId": "95d0dd64-73dd-47cc-8d4e-993b43bd77d0"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, label_map):\n",
    "        self.file_list = file_list\n",
    "#         self.n_class = len(label_map)\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img_pil = torchvision.transforms.Resize((224,224))(img)\n",
    "        img = torchvision.transforms.ToTensor()(img_pil)\n",
    "        if img.shape[0] == 3:\n",
    "            img = torchvision.transforms.Grayscale(num_output_channels=1)(img_pil)\n",
    "            img = torchvision.transforms.ToTensor()(img)\n",
    "        label = self.label_map[self.file_list[index].split('/')[-1].split('.')[0]]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8WkNa73xzd63"
   },
   "outputs": [],
   "source": [
    "dataset = ImageDataset(img_list, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DQWaXqRtzd65"
   },
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset, test_dataset = torch.utils.data.random_split(dataset, (195, 66, 66))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "w2FJRhYXzd66"
   },
   "outputs": [],
   "source": [
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cCO6k3Yfzd69"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, \n",
    "                                               shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Wj46-XYLzd6_"
   },
   "outputs": [],
   "source": [
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=64, \n",
    "                                               shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input1_size, input2_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_fclayer = nn.Linear(input2_size, input1_size)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        \n",
    "        self.input1 = input1\n",
    "        self.input2 = input2\n",
    "        \n",
    "        self.batch, self.outchannel1, self.h1, self.w1 = input1.shape\n",
    "        self.batch, self.outchannel2 = input2.shape\n",
    "        \n",
    "        input2_rescaled = self.attention_fclayer(self.input2)\n",
    "        \n",
    "        input2_bmm = input2_rescaled.view(self.batch,self.outchannel1,1)\n",
    "        \n",
    "        compat_scores = torch.zeros((self.batch, self.h1*self.w1))\n",
    "\n",
    "        for h in range(self.h1):\n",
    "            for w in range(self.w1):\n",
    "                input1_bmm = self.input1[:,:,h,w].view(self.batch,1,self.outchannel1)\n",
    "                compat_scores[:,h*self.w1+w] = torch.bmm(input1_bmm, input2_bmm).squeeze()\n",
    "        \n",
    "        normalized_compat_scores = F.softmax(compat_scores)\n",
    "        \n",
    "        bmm_arg2 = self.input1.view(self.batch,self.outchannel1,self.h1*self.w1,1)\n",
    "        bmm_argtemp = normalized_compat_scores.view(self.batch,1,self.h1*self.w1).repeat(1,self.outchannel1,1)\n",
    "        bmm_arg1 = bmm_argtemp.view(self.batch,self.outchannel1,1,self.h1*self.w1)\n",
    "        \n",
    "        g_mod = torch.zeros((self.batch, self.outchannel1))\n",
    "        \n",
    "        for b in range(self.batch):\n",
    "            g_mod[b,:] = torch.bmm(bmm_arg1[b,:,:,:], bmm_arg2[b,:,:,:]).squeeze()\n",
    "        \n",
    "        return g_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1Wxrwfckzd7B"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "                          nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=kernel_size, stride=stride),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        layers = []\n",
    "        self.num_classes = 13\n",
    "        channels = [1, 64, 128, 256] # this needs to be modified according to num_blocks\n",
    "        \n",
    "        self.convlayer1 = ConvBlock(C_in=channels[0], C_out=channels[1], kernel_size=5, stride=1)\n",
    "        self.convlayer2 = ConvBlock(C_in=channels[1], C_out=channels[2], kernel_size=5, stride=1)\n",
    "        self.convlayer3 = ConvBlock(C_in=channels[2], C_out=channels[3], kernel_size=5, stride=1)\n",
    "        \n",
    "#         for i in range(num_blocks):\n",
    "#             layers.append(ConvBlock(C_in=channels[i], C_out=channels[i+1], kernel_size=5, stride=1))\n",
    "        \n",
    "        self.dropoutlayer1 = nn.Dropout(p=0.25)\n",
    "        \n",
    "#         layers.append(nn.Dropout(p=0.25))\n",
    "        \n",
    "        self.flattenlayer = Flatten()\n",
    "        \n",
    "#         layers.append(Flatten())\n",
    "        \n",
    "        self.fclayer1 = nn.Linear(256*24*24, 512)\n",
    "        \n",
    "#         layers.append(nn.Linear(256*24*24, 512))\n",
    "        \n",
    "        self.dropoutlayer2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.attlayer1 = AttentionLayer(channels[1],512)\n",
    "        self.attlayer2 = AttentionLayer(channels[2],512)\n",
    "        self.attlayer3 = AttentionLayer(channels[3],512)\n",
    "        \n",
    "#         layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "#         fclayer2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "#         layers.append(nn.Linear(512, num_classes))\n",
    "        \n",
    "#         self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out1 = self.convlayer1(x)\n",
    "        self.out2 = self.convlayer2(self.out1)\n",
    "        self.out3 = self.convlayer3(self.out2)\n",
    "        self.out4 = self.dropoutlayer1(self.out3)\n",
    "        self.out5 = self.flattenlayer(self.out4)\n",
    "        self.out6 = self.fclayer1(self.out5)\n",
    "        self.out7 = self.dropoutlayer2(self.out6)\n",
    "        \n",
    "        self.attout1 = self.attlayer1(self.out1,self.out7)\n",
    "        self.attout2 = self.attlayer2(self.out2,self.out7)\n",
    "        self.attout3 = self.attlayer3(self.out3,self.out7)\n",
    "        \n",
    "        self.attout = torch.cat((self.attout1,self.attout2,self.attout3),1)\n",
    "        \n",
    "        _, self.att_size = self.attout.shape\n",
    "        \n",
    "        self.fclayer2 = nn.Linear(self.att_size, self.num_classes)\n",
    "        self.final_out = self.fclayer2(self.attout)\n",
    "        \n",
    "        return self.attout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6150,
     "status": "ok",
     "timestamp": 1572056426800,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "re2Kdhiazd7C",
    "outputId": "bfb5ad37-62c5-4bf4-e910-452f67ca8b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = BaselineModel(num_blocks=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "# print(model)\n",
    "# print(model(dataset[0][0].unsqueeze(dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kRLEOoHdzd7E"
   },
   "outputs": [],
   "source": [
    "def train(model,n_epochs,train_dataloader, test_loader):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(train_dataloader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                logging.debug('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        train_losses.append(avg_loss)\n",
    "        test_loss, test_accuracy = test_classify_loss(model,test_loader)\n",
    "        eval_losses.append(test_loss)\n",
    "        eval_accs.append(test_accuracy)\n",
    "        print('Epoch: {}\\tTrain Loss: {}\\tTest-Loss: {}\\tTest-acc: {:.4f}'.format(epoch+1, avg_loss, test_loss, test_accuracy))\n",
    "    return train_losses, eval_losses, eval_accs\n",
    "\n",
    "def test_classify_loss(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        accuracies = []\n",
    "        total = 0\n",
    "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            outputs = model(feats)\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            accuracies += [float(torch.sum(torch.eq(pred_labels, labels)).item())/float(len(labels))]\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30341,
     "status": "error",
     "timestamp": 1572056451006,
     "user": {
      "displayName": "Tejasri Thota",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA-qo0IWRkoBECjG3L_eRQ7NjBOkZxcaesklum9=s64",
      "userId": "10780644812533486358"
     },
     "user_tz": 420
    },
    "id": "ACkvD4NVzd7G",
    "outputId": "9e3fc8f9-69ae-45f6-de28-8bdbbeb73637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "here3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumeet/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here4\n",
      "here5\n"
     ]
    }
   ],
   "source": [
    "train_losses, eval_losses, eval_accs = train(model,200, train_dataloader,dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6AXVsznzd7J"
   },
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses)\n",
    "plt.savefig(\"training_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(eval_accs)\n",
    "plt.savefig(\"val_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(1,2,3,4)\n",
    "print(x.view(1,2,12,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_cnn (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
